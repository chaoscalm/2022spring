{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCP Examples - Linear Programming (LP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Convex, COSMO, CairoMakie, Gurobi, MathOptInterface, MosekTools, Random, SCS\n",
    "const MOI = MathOptInterface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear programming (LP)\n",
    "\n",
    "* A general linear program takes the form\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& \\mathbf{c}^T \\mathbf{x} \\\\\n",
    "\t&\\text{subject to}& \\mathbf{A} \\mathbf{x} = \\mathbf{b} \\\\\n",
    "\t& & \\mathbf{G} \\mathbf{x} \\preceq \\mathbf{h}.\n",
    "\\end{eqnarray*}\n",
    "Linear program is a convex optimization problem, why?\n",
    "\n",
    "<img src=\"./lp.png\" width=\"300\" align=\"center\"/>\n",
    "\n",
    "* The **standard form** of an LP is\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& \\mathbf{c}^T \\mathbf{x} \\\\\n",
    "\t&\\text{subject to}& \\mathbf{A} \\mathbf{x} = \\mathbf{b} \\\\\n",
    "\t& & \\mathbf{x} \\succeq \\mathbf{0}.\n",
    "\\end{eqnarray*}\n",
    "To transform a general linear program into the standard form, we introduce the _slack variables_ $\\mathbf{s} \\succeq \\mathbf{0}$ such that $\\mathbf{G} \\mathbf{x} + \\mathbf{s} = \\mathbf{h}$. Then we write $\\mathbf{x} = \\mathbf{x}^+ - \\mathbf{x}^-$, where $\\mathbf{x}^+ \\succeq \\mathbf{0}$ and $\\mathbf{x}^- \\succeq \\mathbf{0}$. This yields the problem\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& \\mathbf{c}^T (\\mathbf{x}^+ - \\mathbf{x}^-) \\\\\n",
    "\t&\\text{subject to}& \\mathbf{A} (\\mathbf{x}^+ - \\mathbf{x}^-) = \\mathbf{b} \\\\\n",
    "\t& & \\mathbf{G} (\\mathbf{x}^+ - \\mathbf{x}^-) + \\mathbf{s} = \\mathbf{h} \\\\\n",
    "\t& & \\mathbf{x}^+ \\succeq \\mathbf{0}, \\mathbf{x}^- \\succeq \\mathbf{0}, \\mathbf{s} \\succeq \\mathbf{0}\n",
    "\\end{eqnarray*}\n",
    "in $\\mathbf{x}^+$, $\\mathbf{x}^-$, and $\\mathbf{s}$.\n",
    "\n",
    "    Slack variables are often used to transform a complicated inequality constraint to simple non-negativity constraints.\n",
    "\n",
    "* The **inequality form** of an LP is\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& \\mathbf{c}^T \\mathbf{x} \\\\\n",
    "\t&\\text{subject to}& \\mathbf{G} \\mathbf{x} \\preceq \\mathbf{h}.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "* Some softwares, e.g., `solveLP` in R, require an LP be written in either standard or inequality form. However a good software should do this for you!\n",
    "\n",
    "* A _piecewise-linear minimization_ problem\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& \\max_{i=1,\\ldots,m} (\\mathbf{a}_i^T \\mathbf{x} + b_i)\n",
    "\\end{eqnarray*}\n",
    "can be transformed to an LP\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& t \\\\\n",
    "\t&\\text{subject to}& \\mathbf{a}_i^T \\mathbf{x} + b_i \\le t, \\quad i = 1,\\ldots,m,\n",
    "\\end{eqnarray*}\n",
    "in $\\mathbf{x}$ and $t$. Apparently \n",
    "$$\n",
    "\t\\text{minimize} \\max_{i=1,\\ldots,m} |\\mathbf{a}_i^T \\mathbf{x} + b_i|\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\t\\text{minimize} \\max_{i=1,\\ldots,m} (\\mathbf{a}_i^T \\mathbf{x} + b_i)_+\n",
    "$$\n",
    "are also LP.\n",
    "\n",
    "* Any _convex optimization problem_\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& f_0(\\mathbf{x}) \\\\\n",
    "\t&\\text{subject to}& f_i(\\mathbf{x}) \\le 0, \\quad i=1,\\ldots,m \\\\\n",
    "\t&& \\mathbf{a}_i^T \\mathbf{x} = b_i, \\quad i=1,\\ldots,p,\n",
    "\\end{eqnarray*}\n",
    "where $f_0,\\ldots,f_m$ are convex functions, can be transformed to the _epigraph form_\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& t \\\\\n",
    "\t&\\text{subject to}& f_0(\\mathbf{x}) - t \\le 0 \\\\\n",
    "\t& & f_i(\\mathbf{x}) \\le 0, \\quad i=1,\\ldots,m \\\\\n",
    "\t& & \\mathbf{a}_i^T \\mathbf{x} = b_i, \\quad i=1,\\ldots,p\n",
    "\\end{eqnarray*}\n",
    "in variables $\\mathbf{x}$ and $t$. That is why people often say linear program is universal.\n",
    "\n",
    "* The _linear fractional programming_\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& \\frac{\\mathbf{c}^T \\mathbf{x} + d}{\\mathbf{e}^T \\mathbf{x} + f} \\\\\n",
    "\t&\\text{subject to}& \\mathbf{A} \\mathbf{x} = \\mathbf{b} \\\\\n",
    "\t& & \\mathbf{G} \\mathbf{x} \\preceq \\mathbf{h} \\\\\n",
    "\t& & \\mathbf{e}^T \\mathbf{x} + f > 0\n",
    "\\end{eqnarray*}\n",
    "can be transformed to an LP\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& \\mathbf{c}^T \\mathbf{y} + d z \\\\\n",
    "\t&\\text{subject to}& \\mathbf{G} \\mathbf{y} - z \\mathbf{h}  \\preceq \\mathbf{0} \\\\\n",
    "\t& & \\mathbf{A} \\mathbf{y} - z \\mathbf{b} = \\mathbf{0} \\\\\n",
    "\t& & \\mathbf{e}^T \\mathbf{y} + f z = 1 \\\\\n",
    "\t& & z \\ge 0\n",
    "\\end{eqnarray*}\n",
    "in $\\mathbf{y}$ and $z$, via transformation of variables\n",
    "\\begin{eqnarray*}\n",
    "\t\\mathbf{y} = \\frac{\\mathbf{x}}{\\mathbf{e}^T \\mathbf{x} + f}, \\quad z = \\frac{1}{\\mathbf{e}^T \\mathbf{x} + f}.\n",
    "\\end{eqnarray*}\n",
    "See Section 4.3.2 of Boyd and Vandenberghe (2004) for proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LP example: compressed sensing\n",
    "\n",
    "* **Compressed sensing** [Candes and Tao (2006)](https://doi.org/10.1109/TIT.2006.885507) and [Donoho (2006)](https://doi.org/10.1109/TIT.2006.871582) tries to address a fundamental question: how to compress and transmit a complex signal (e.g., musical clips, mega-pixel images), which can be decoded to recover the original signal?\n",
    "\n",
    "<img src=\"./david-donoho.jpg\" width=\"100\"/>\n",
    "<img src=\"./emmanuel-candes.png\" width=\"100\"/>\n",
    "<img src=\"./terrence-tao.png\" width=\"100\"/>\n",
    "\n",
    "* Suppose a signal $\\mathbf{x} \\in \\mathbb{R}^n$ is sparse with $s$ non-zeros. We under-sample the signal by multiplying a (flat) measurement matrix $\\mathbf{y} = \\mathbf{A} \\mathbf{x}$, where $\\mathbf{A} \\in \\mathbb{R}^{m\\times n}$ has iid normal entries. [Candes, Romberg and Tao (2006)](https://doi.org/10.1002/cpa.20124) show that the solution to\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& \\|\\mathbf{x}\\|_1 \\\\\n",
    "\t&\\text{subject to}& \\mathbf{A} \\mathbf{x} = \\mathbf{y}\n",
    "\\end{eqnarray*}\n",
    "exactly recovers the true signal under certain conditions on $\\mathbf{A}$ when $n \\gg s$ and $m \\approx s \\ln(n/s)$. Why sparsity is a reasonable assumption? _Virtually all real-world images have low information content_.\n",
    "\n",
    "<img src=\"./movie-scene.png\" width=\"400\"/>\n",
    "\n",
    "* The $\\ell_1$ minimization problem apparently is an LP, by writing $\\mathbf{x} = \\mathbf{x}^+ - \\mathbf{x}^-$,\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& \\mathbf{1}^T (\\mathbf{x}^+ + \\mathbf{x}^-)  \\\\\n",
    "\t&\\text{subject to}& \\mathbf{A} (\\mathbf{x}^+ - \\mathbf{x}^-) = \\mathbf{y} \\\\\n",
    "\t& & \\mathbf{x}^+ \\succeq \\mathbf{0}, \\mathbf{x}^- \\succeq \\mathbf{0}.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "* Let's try a numerical example.\n",
    "\n",
    "### Generate a sparse signal and sub-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "Random.seed!(123)\n",
    "# Size of signal\n",
    "n = 1024\n",
    "# Sparsity (# nonzeros) in the signal\n",
    "s = 20\n",
    "# Number of samples (undersample by a factor of 8) \n",
    "m = 128\n",
    "\n",
    "# Generate and display the signal\n",
    "x0 = zeros(n)\n",
    "x0[rand(1:n, s)] = randn(s)\n",
    "# Generate the random sampling matrix\n",
    "A = randn(m, n) / m\n",
    "# Subsample by multiplexing\n",
    "y = A * x0\n",
    "\n",
    "# plot the true signal\n",
    "f = Figure()\n",
    "Axis(\n",
    "    f[1, 1], \n",
    "    title = \"True Signal x0\",\n",
    "    xlabel = \"x\",\n",
    "    ylabel = \"y\"\n",
    ")\n",
    "lines!(1:n, x0)\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve LP by DCP (disciplined convex programming) interface Convex.jl\n",
    "\n",
    "Check [Convex.jl documentation](https://www.juliaopt.org/Convex.jl/stable/operations/) for a list of supported operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use Mosek solver\n",
    "# solver = Mosek.Optimizer()\n",
    "# MOI.set(solver, MOI.RawOptimizerAttribute(\"LOG\"), 1)\n",
    "\n",
    "# # Use Gurobi solver\n",
    "# solver = Gurobi.Optimizer()\n",
    "# MOI.set(solver, MOI.RawOptimizerAttribute(\"OutputFlag\"), 1)\n",
    "\n",
    "# Use COSMO solver\n",
    "solver = COSMO.Optimizer()\n",
    "# MOI.set(solver, MOI.RawOptimizerAttribute(\"max_iter\"), 5000)\n",
    "\n",
    "# # Use SCS solver\n",
    "# solver = SCS.Optimizer()\n",
    "# MOI.set(solver, MOI.RawOptimizerAttribute(\"verbose\"), 1)\n",
    "\n",
    "# Set up optimizaiton problem\n",
    "x = Variable(n)\n",
    "problem = minimize(norm(x, 1))\n",
    "problem.constraints += A * x == y\n",
    "\n",
    "# Solve the problem\n",
    "@time solve!(problem, solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the solution\n",
    "f = Figure()\n",
    "Axis(\n",
    "    f[1, 1], \n",
    "    title = \"Reconstructed signal overlayed with x0\",\n",
    "    xlabel = \"x\",\n",
    "    ylabel = \"y\"\n",
    ")\n",
    "scatter!(1:n, x0, label = \"truth\")\n",
    "lines!(1:n, vec(x.value), label = \"recovery\")\n",
    "axislegend(position = :lt)\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LP example: quantile regression\n",
    "\n",
    "<img src=\"./growth-2-20-boys.png\" width=\"300\"/>\n",
    "<img src=\"./quantregbin3.png\" width=\"300\"/>\n",
    "\n",
    "* In linear regression, we model the mean of response variable as a function of covariates. In many situations, the error variance is not constant, the distribution of $y$ may be asymmetric, or we simply care about the quantile(s) of response variable. Quantile regression offers a better modeling tool in these applications.\n",
    "\n",
    "\n",
    "* In $\\tau$-quantile regression, we minimize the loss function\n",
    "\\begin{eqnarray*}\n",
    "\tf(\\beta) = \\sum_{i=1}^n \\rho_\\tau (y_i - \\mathbf{x}_i^T \\beta),\n",
    "\\end{eqnarray*}\n",
    "where $\\rho_\\tau(z) = z (\\tau - 1_{\\{z < 0\\}})$. Writing $\\mathbf{y}  - \\mathbf{X} \\beta = \\mathbf{r}^+ - \\mathbf{r}^-$, this is equivalent to the LP\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& \\tau \\mathbf{1}^T \\mathbf{r}^+ + (1-\\tau) \\mathbf{1}^T \\mathbf{r}^- \\\\\n",
    "\t&\\text{subject to}& \\mathbf{r}^+ - \\mathbf{r}^- = \\mathbf{y} - \\mathbf{X} \\beta \\\\\n",
    "\t& & \\mathbf{r}^+ \\succeq \\mathbf{0}, \\mathbf{r}^- \\succeq \\mathbf{0}\n",
    "\\end{eqnarray*}\n",
    "in $\\mathbf{r}^+$, $\\mathbf{r}^-$, and $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LP Example: $\\ell_1$ regression\n",
    "\n",
    "* A popular method in robust statistics is the median absolute deviation (MAD) regression that minimizes the $\\ell_1$ norm of the residual vector $\\|\\mathbf{y} - \\mathbf{X} \\beta\\|_1$. This apparently is equivalent to the LP\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& \\mathbf{1}^T (\\mathbf{r}^+ + \\mathbf{r}^-) \\\\\n",
    "\t&\\text{subject to}& \\mathbf{r}^+ - \\mathbf{r}^- = \\mathbf{y} - \\mathbf{X} \\beta \\\\\n",
    "\t& & \\mathbf{r}^+ \\succeq \\mathbf{0}, \\mathbf{r}^- \\succeq \\mathbf{0}\n",
    "\\end{eqnarray*}\n",
    "in $\\mathbf{r}^+$, $\\mathbf{r}^-$, and $\\beta$. \n",
    "\n",
    "    $\\ell_1$ regression = MAD = 1/2-quantile regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LP Example: $\\ell_\\infty$ regression (Chebychev approximation)\n",
    "\n",
    "* Minimizing the worst possible residual $\\|\\mathbf{y} - \\mathbf{X} \\beta\\|_\\infty$ is equivalent to the LP\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& t \\\\\n",
    "\t&\\text{subject to}& -t \\le y_i - \\mathbf{x}_i^T \\beta \\le t, \\quad i = 1,\\dots,n\n",
    "\\end{eqnarray*}\n",
    "in variables $\\beta$ and $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LP Example: Dantzig selector\n",
    "\n",
    "* [Candes and Tao (2007)](https://www.doi.org/10.1214/009053606000001523) propose a variable selection method called the Dantzig selector that solves\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& \\|\\mathbf{X}^T (\\mathbf{y} - \\mathbf{X} \\beta)\\|_\\infty \\\\\n",
    "\t&\\text{subject to}& \\sum_{j=2}^p |\\beta_j| \\le t,\n",
    "\\end{eqnarray*}\n",
    "which can be transformed to an LP. Indeed they name the method after George Dantzig, who invented the simplex method for efficiently solving LP in 50s.\n",
    "\n",
    "<img src=\"./george-bernard-dantzig.jpg\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LP Example: 1-norm SVM\n",
    "\n",
    "* In two-class classification problems, we are given training data $(\\mathbf{x}_i, y_i)$, $i=1,\\ldots,n$, where $\\mathbf{x}_i \\in \\mathbb{R}^p$ are feature vectors and $y_i \\in \\{-1, 1\\}$ are class labels. [Zhu, Rosset, Tibshirani, and Hastie (2004)](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=2ahUKEwi-_6C2sMLiAhUNGDQIHU1RD3MQFjAAegQIBRAC&url=https%3A%2F%2Fpapers.nips.cc%2Fpaper%2F2450-1-norm-support-vector-machines.pdf&usg=AOvVaw1cTOsmPjpxKJHCs93iLUkn) propose the 1-norm support vector machine (svm) that achieves the dual purpose of classification and feature selection. Denote the solution of the optimization problem\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& \\sum_{i=1}^n \\left[ 1 - y_i \\left( \\beta_0 + \\sum_{j=1}^p x_{ij} \\beta_j \\right) \\right]_+ \\\\\n",
    "\t&\\text{subject to}& \\|\\beta\\|_1 = \\sum_{j=1}^p |\\beta_j| \\le t\n",
    "\\end{eqnarray*}\n",
    "by $\\hat \\beta_0(t)$ and $\\hat \\beta(t)$. 1-norm svm classifies a future feature vector $\\mathbf{x}$ by the sign of fitted model\n",
    "\\begin{eqnarray*}\n",
    "\t\\hat f(\\mathbf{x}) = \\hat \\beta_0 + \\mathbf{x}^T \\hat \\beta.\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many more applications of LP: Airport scheduling (Copenhagen airport uses Gurobi), airline flight scheduling, NFL scheduling, match.com, $\\LaTeX$, ...\n",
    "\n",
    "Apparently any loss/penalty or loss/constraint combinations of form \n",
    "$$\n",
    "\\{\\ell_1, \\ell_\\infty, \\text{quantile}\\} \\times \\{\\ell_1, \\ell_\\infty, \\text{quantile}\\},\n",
    "$$\n",
    "possibly with affine (equality and/or inequality) constraints, can be formulated as an LP."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.7.1",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "30.66666603088379px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "530px",
    "left": "0px",
    "right": "837px",
    "top": "109.4000015258789px",
    "width": "123px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
